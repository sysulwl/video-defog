场景深度   								scene depth
立体匹配   								stereo matching 
雾信息	   								fog information
光相干项(照片一致性项)，光密度测量   	photo-consistency term
散射效应   								scattering effects
先验消光拉普拉斯约束   					prior matting Laplacian constraint
细节保持平滑约束						detail-preserving smoothness constraint
MRF框架
辅助变量								auxiliary variables
迭代优化								optimized iteratively 
多视角立体								Multi-view stereo
图像对比度								image contrast

深度平滑								depth smoothness
表面细节								surface details
depth ordering
detail preserving smoothness
airlight-albedo
unsaturated color
multiple depth map approach
multi-view stereo (MVS) vision 
visibility
occlusions
consistency 
sensitivity
geometric coherence term
geometric constraints
characteristic artifacts
energy functions
 depth maps
 shrinking bias
 airlight
 maximizing local contrast
shading 
transmission functions
dark channe
boundary constraint
image fusion
photo-consistency term
  
atmospheric light    是什么  ， alpha是什么？
如何获取alpha ，  atomspheric light A呢。
首先


estTransmission  这个函数，生成了A之后，将im的r g b 三维点除以A， 再简单计算得到alpha矩阵。这就是我们看到的第一张灰度图。

随后，进入拉普拉斯函数的研究。
  
有非常多的专业术语需要我们去搞懂，所以建议还是用英文去学习吧。 
首先，深度图是啥？————是不是我们传统理解的灰度图?
原来，深度就是离我们多远的这个概念，来决定一张图中的先后顺序。深度图像的每个像素点的灰度值可用于表征场景中某一点距离摄像机的远近

边缘检测的目的是标识数字图像中属性显著变化的点。图像属性中的显著变化通常反映了属性的重要变化。这些包括：
    深度上的不连续
    表面法线方向不连续
    颜色不连续
    亮度不连续
边缘是灰度值不连续的结果，这种不连续常可利用求导数方便地检测到，一般常用一阶和二阶导数来检测边缘。
其中一阶导数的幅度值来检测边缘的存在，幅度峰值一般对应边缘位置。

获取场景中各点相对于摄象机的距离是计算机视觉系统的重要任务之一．场景中各点相对于摄象机的距离可以用深度图(Depth Map)来表示，
即深度图中的每一个像素值表示场景中某一点与摄像机之间的距离．机器视觉系统获取场景深度图技术可分为 被动测距传感 和 主动深度传感 两大类．
被动测距传感是指视觉系统接收来自场景发射或反射的光能量，形成有关 场景光能量分布函数， 即灰度图像，然后在这些图像的基础上恢复场景的深度信息．

最一般的方法是使用两个相隔一定距离的摄像机同时获取场景图像来生成深度图．
与此方法相类似的另一种方法是一个摄象机在不同空间位置上获取两幅或两幅以上图像，通过多幅图像的灰度信息和成象几何来生成深度图．

第3章 实验过程:
(1)得到每一帧的深度图Z，从而得到逆深度图D------------->那么第一个问题，深度图是什么？ 如何求深度图？参考的是谁的方法？
(2)构造能量方程，目标是使得能量和最小
能量函数的唯一参数是Dt, 是逆深度图， 也就是深度图的倒数，


(3)搞清楚能量方程的每一项 
第一项， Photo-consistency term
首先，要清楚 Photo-consistency 是什么?——————参考  https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-31439-6_204
定义: 
Photo-consistency是一种标量函数，用于测量3D重建与一组校准图像的视觉兼容性。
背景: 
多年来，图像自动三维重建一直是核心计算机视觉问题。多视点立体（MVS）是从多个图像重建物体或场景的三维结构的过程[1]。
MVS采用经过校准的照片，其中照相机校准通常通过校准图（例如，棋盘图案）或运动结构算法[2]来实现。原则上，MVS算法如何恢复3D
信息与人类用双眼感知深度的方式相同，即来自对应的三角测量。因此，MVS的关键的第一步是建立跨多个输入图像的特征对应关系，
其中需要强大的机制来评估这些特征对应关系的好处，这是Photo-consistency函数的作用。从某种意义上说，MVS重建过程是在三维
空间中挖掘出二维表面，其中Photo-consistency的值较高。（越高说明越不匹配）
理论和样例：
光照一致性f（p，V）是一种标量函数，用于测量给定三维重建p与一组图像的视觉兼容性。
通常，p是一个三维点（p \ in \ mathbb {R} ^ 3），————（实际中的3D坐标是怎么定义的呢，我的理解是深度）
在3D点p处的简单Photo-consistency函数被定义如下：将p投影到V中的每个可见图像中，并且将它们投影附近的图像纹理的相似度计算为Photo-consistency。
不是比较每个图像中的单个像素颜色，而是比较每个局部图像区域中的一组像素颜色的鲁棒性。


几何相干项的章节太少了
平滑项的内容也比较少
拉普拉斯项的内容也比较少
这三项如果有图就会好办一点。

参考论文[43]: 几何相干项:
与典型的多视点立体方法不同，我们的方法不仅强加颜色一致性约束，而且还以统计方式将几何相关性与多个帧明确关联。
将不同视图关联的光照一致性和几何一致性约束组合在全局能量最小化框架中。
它们有助于可靠地减少图像噪声和多帧数据遮挡的影响，从而使我们的优化不受过度平滑或混合伪影的影响。
对于一些使用水平集或可变形多边形网格的MVS方法[9]，[49]，几何相干约束以3D方式并入和制定。
相比之下，我们的方法统计数据术语定义中的光致一致性和几何相干性约束。
通过结合光照一致性和几何一致性约束，我们的数据成本分布变得与众不同，从而使BP优化更加稳定和快速收敛。
让计算机自动建立多幅图像之间的匹配关系（如找到p1点的对应点p2）其实是三维重建最困难的一个问题。图像匹配最常见的做法是基于特征点（如Harris、DoG特征点）的匹配，
此外还有基于直线的匹配，以及基于区域的匹配。
 
空间点P不仅可以通过在左右两张图像上的投影点p1和p2来重建（这称之为双视图几何），类似地，还可通过同时拍3张图像来重建点P，
这就被称为三视图几何。其中三焦张量（Trifocal Tensor）等同于双视图几何中基本矩阵的地位，类推还有四视图几何的四焦张量。
3视图及以上的几何重建统称为多视图立体重建（MVS，Multi-View Stereo）。

平滑项:
Kang和Szeliski [43里面的19]提出通过添加时间平滑项同时优化多个关键帧处的一组深度图。
我们的自适应平滑项在平坦区域中保持平滑，同时保留纹理边缘。
在[43里面的19]中，在数据项之外引入了一种外部平滑项，其功能类似于空间平滑度约束。
在实际情况中，基于逐像素点的匹配代价并不能完全正确地反映两幅图像中两个点匹配的正确性；比如噪声、大范围的相似区域等，其结果是错误匹配的代价常常会小于正确匹配代价，
从而影响算法在该点的深度估计。 并且在全局算法尤其是动态规划算法的框架下，这样的错误匹配代价的估算往往会影响到周围点的深度估计，进而将错误扩散［１］。 因此，在半
全局匹配方法中，必须增加一些额外的平滑约束到能量的定义中，这种约束通常是采用对深度或者灰度的变化的惩罚，以抑制噪声对匹配结果的影响

拉普拉斯平滑项:
雾的出现也开启了丰富重建深度细节的可能性。 为此，我们将拉普拉斯[21]约束作为细节保留平滑项。

3.1  颜色一致性项的构建
3.1.1 评估深度图的颜色一致性
3.1.2 基于散射效应的颜色一致性项的计算

几何相关项的构建

平滑项的构建
朴素的平滑
基于传输的排序约束的平滑

3.4  拉普拉斯平滑项的构建

立体重建深度图的构建
能量方程的构建
能量方程的最优化

3.6  本章小结

vs全部代码排版：1. ctrl+E,D 按住ctrl，相继按下E、D
2. ctrl+K,F 按住ctrl，相继按下K、F

size_t在64位是unsigned long long , 转成int就会有警告

实验结果分析的翻译:
	在我们的实验中，预先估计所有数据中所有帧的相机参数。由于散射效应，我们使用SIFT检测器和描述符[22]来匹配视频中的特征点。我们发现大多数特征都是从靠近物体提取的，这
些物体不会被散射媒体降解，因此大多数匹配的特征都很有信心。然后我们使用[18]中提出的SfM方法来恢复相机姿态。利用估计的相机姿态，我们首先通过求解没有几何相干项的方程
（9）来进行深度初始化步骤，然后在下一步中求解方程（9）的完整版本。方程（9）（有或没有几何相干项）通过迭代求解方程（16）和方程（17）来求解。在深度初始化步骤中，我
们尚未使用雾传输图，所以暗通道方法[17]的结果被用作α的初始化以应用排序约束。在随后的迭代中，通过等式（17）从所估计的深度图中保持雾透射图的更新。
我们评估我们的方法在几个具有挑战性的视频这些视频是在雾天拍摄的。在这些视频中，“Blenheim”数据从Vimeo下载。我们还使用水下摄像头在坦克中拍摄到混浊的水下视频。为了模
拟散射介质，我们将牛奶倒入水中。完整的视频结果可以在补充材料中找到。所有的实验都在配备英特尔四核2.4GHz CPU的台式机上运行。处理具有480×270图像分辨率的帧所用的时间
约为10分钟。
	对于立体评估，我们与先进的基于视频的立体重建进行比较[43]。对于除雾评估，我们将其与几种现有的单图像去雾方法进行比较，即He等人的暗通道方法。 [17]，Meng等人的最新算
法。 [24]和[36]的改进版本，记为NBPC + PA [37]。为了更公平的比较，我们也评估采用雾和立体声提示的方法;这些包括我们自己实施的Caraffa和Tarel（简称CT）[7]以及除雾和立
体两种直接组合（类似于[27,30]）。直接组合简单地在不同的初始化之间迭代（Meng等[24]）和立体（由Zhang等[43]）去雾。我们用立体声结果作为SD（立体声然后除雾），另一个
作为DS（除雾然后立体声）来表示。当结果没有大的变化时（通常是5次迭代），迭代停止。
	图5显示了合成数据的立体估计评估。 为了比较，估计的逆深度图被标准化为[0,1]。 可以观察到，我们的结果非常接近于近地和远地的基本事实，而Zhang等人 由于低对比度，[43]
无法很好地区分地面和墙壁的深度。 因此，这些区域错误地分配了接近的反向深度。
	对于合成数据的除雾评估，在图6中观察到，我们的方法根据误差图产生与地面真实最接近的结果。 相反，Meng等人 [24]（其他两种单幅图像除雾方法的结果相似）在地面区域显示出
明显的误差，该区域颜色浅且因此被错误地（由于各自的先验采用）作为遥远的物体密集地覆盖着强 多雾路段。 通过这些基于单个图像的方法，图像中间的远处区域的雾没有被完全
去除。 此外，单个图像去雾方法会在视频上产生时间闪烁，因为每个帧都是独立处理的。 请参阅补充视频
	然后，我们将我们的算法与真实世界数据上的其他算法进行比较，如图7所示。仅比较立体算法[43]（第二列，每一个偶数行），从图7可以看出[43]无法恢复被散射介质高度降解的远
处物体的深度，例如“巴厘岛”中的房屋，“布莱尼姆”中的远处树木以及“水下”中的城堡。相反，我们的方法更准确地恢复了房屋，树木和城堡的结构。此外，[43]在这些数据中产生了
一些文物，如“巴厘岛”的植物区域，“布伦海姆”的天空区域，“摩托车”的门区域以及“水下”的胸部附近区域。相比之下，我们的方法没有这种文物。我们的方法保留了现场的细节，例
如嵌入“巴厘岛”前景混凝土柱中的细长钢架，“布莱尼姆”中树木和灌木的更复杂的几何细节，以及“水下” ”。
	仅比较去雾算法[24]（第二列，每奇数行），从图7可以看出，我们的方法能够恢复更加忠实的色彩，特别是在“摩托车”和“ 操场“序列，这再次导致单个图像除雾的问题。 此外，“摩
托车”右侧门前的雾没有被[24]完全消除，“水下”的恢复场景更暗。 相比之下，我们的方法很好地处理这些区域，这要归功于我们的立体算法估计的高质量深度。
	与去雾和立体两种直接组合相比，从图7可以看出，分离率去雾和立体声不一定相互提高。 对于DS，尽管最初的除雾图像看起来在视觉上似乎是合理的（如图7的第二列所示），但实际
上它们在帧上不一致并导致较差的立体效果。 例如，“巴厘岛”附近柱子和“布伦海姆”天空的立体结果变得更糟。 可持续发展方法遭受类似的问题。 由于类似的不一致性问题，其在“
Bali”和“Blenheim”上的立体声效果在迭代后变差。 他们也无法恢复远处物体的深度和形状细节
	与CT [7]相比，从图7可以看出，我们的方法在除雾和立体声方面均优于其他方法。 他们的方法仍然使用传统的照片一致性术语作为立体声初始化，这在散射媒体中变得不太有效
。 他们对立体声和除雾术语的简单添加不能重建场景的远处物体和深度细节。 由于散射的直接反比对辐射定标敏感，所以它们的去雾效果与其他的相比也更差。